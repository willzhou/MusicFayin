# Author: Ningbo Wise Effects, Inc. (æ±‡è§†åˆ›å½±) & Will Zhou
# Date: 2025-07-07
# License: Apache 2.0

import streamlit as st
import json
import requests
from datetime import datetime
import os
import subprocess
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch import Tensor

import torchaudio
import numpy as np
from omegaconf import OmegaConf
from typing import Dict, Any, List, Tuple, Optional
import psutil
import sys
from pathlib import Path
import re

# å¸¸é‡å®šä¹‰
DEEPSEEK_API_KEY = "sk-" # æ¢æˆä½ è‡ªå·±çš„API KEY
DEEPSEEK_URL = "https://api.deepseek.com/v1/chat/completions"

# â€œæ‚²ä¼¤çš„â€ã€â€œæƒ…ç»ªçš„â€ã€â€œæ„¤æ€’çš„â€ã€â€œå¿«ä¹çš„â€ã€â€œä»¤äººæŒ¯å¥‹çš„â€ã€â€œå¼ºçƒˆçš„â€ã€â€œæµªæ¼«çš„â€ã€â€œå¿§éƒçš„â€
EMOTIONS = [
    "sad", "emotional", "angry", "happy", 
    "uplifting", "intense", "romantic", "melancholic"
]

SINGER_GENDERS = ["male", "female"]

# â€œè‡ªåŠ¨â€ã€â€œä¸­å›½ä¼ ç»Ÿâ€ã€â€œé‡‘å±â€ã€â€œé›·é¬¼â€ã€â€œä¸­å›½æˆæ›²â€ã€â€œæµè¡Œâ€ã€â€œç”µå­â€ã€â€œå˜»å“ˆâ€ã€â€œæ‘‡æ»šâ€ã€
# â€œçˆµå£«â€ã€â€œè“è°ƒâ€ã€â€œå¤å…¸â€ã€â€œè¯´å”±â€ã€â€œä¹¡æ‘â€ã€â€œç»å…¸æ‘‡æ»šâ€ã€â€œç¡¬æ‘‡æ»šâ€ã€â€œæ°‘è°£â€ã€â€œçµé­‚ä¹â€ã€
# â€œèˆæ›²ç”µå­â€ã€â€œä¹¡æ‘æ‘‡æ»šâ€ã€â€œèˆæ›²ã€èˆæ›²æµè¡Œã€æµ©å®¤ã€æµè¡Œâ€ã€â€œé›·é¬¼â€ã€â€œå®éªŒâ€ã€â€œèˆæ›²ã€
# æµè¡Œâ€ã€â€œèˆæ›²ã€æ·±æµ©å®¤ã€ç”µå­â€ã€â€œéŸ©å›½æµè¡ŒéŸ³ä¹â€ã€â€œå®éªŒæµè¡Œâ€ã€â€œæµè¡Œæœ‹å…‹â€ã€â€œæ‘‡æ»šä¹â€ã€
# â€œèŠ‚å¥å¸ƒé²æ–¯â€ã€â€œå¤šæ ·â€ã€â€œæµè¡Œæ‘‡æ»šâ€
GENRES = [
    'Auto', 'Chinese Tradition', 'Metal', 'Reggae', 'Chinese Opera',
    "pop", "electronic", "hip hop", "rock", "jazz", "blues", "classical",
    "rap", "country", "classic rock", "hard rock", "folk", "soul",
    "dance, electronic", "rockabilly", "dance, dancepop, house, pop",
    "reggae", "experimental", "dance, pop", "dance, deephouse, electronic",
    "k-pop", "experimental pop", "pop punk", "rock and roll", "R&B",
    "varies", "pop rock",
]

# â€œåˆæˆå™¨ä¸é’¢ç´â€ï¼Œâ€œé’¢ç´ä¸é¼“â€ï¼Œâ€œé’¢ç´ä¸åˆæˆå™¨â€ï¼Œ
# â€œåˆæˆå™¨ä¸é¼“â€ï¼Œâ€œé’¢ç´ä¸å¼¦ä¹â€ï¼Œâ€œå‰ä»–ä¸é¼“â€ï¼Œ
# â€œå‰ä»–ä¸é’¢ç´â€ï¼Œâ€œé’¢ç´ä¸ä½éŸ³æç´â€ï¼Œâ€œé’¢ç´ä¸å‰ä»–â€ï¼Œ
# â€œåŸå£°å‰ä»–ä¸é’¢ç´â€ï¼Œâ€œåŸå£°å‰ä»–ä¸åˆæˆå™¨â€ï¼Œ
# â€œåˆæˆå™¨ä¸å‰ä»–â€ï¼Œâ€œé’¢ç´ä¸è¨å…‹æ–¯é£â€ï¼Œâ€œè¨å…‹æ–¯é£ä¸é’¢ç´â€ï¼Œ
# â€œé’¢ç´ä¸å°æç´â€ï¼Œâ€œç”µå‰ä»–ä¸é¼“â€ï¼Œâ€œåŸå£°å‰ä»–ä¸é¼“â€ï¼Œ
# â€œåˆæˆå™¨â€ï¼Œâ€œå‰ä»–ä¸å°æç´â€ï¼Œâ€œå‰ä»–ä¸å£ç´â€ï¼Œ
# â€œåˆæˆå™¨ä¸åŸå£°å‰ä»–â€ï¼Œâ€œèŠ‚æ‹â€ï¼Œâ€œé’¢ç´â€ï¼Œ
# â€œåŸå£°å‰ä»–ä¸å°æç´â€ï¼Œâ€œé“œç®¡ä¸é’¢ç´â€ï¼Œâ€œè´æ–¯ä¸é¼“â€ï¼Œ
# â€œå°æç´â€ï¼Œâ€œåŸå£°å‰ä»–ä¸å£ç´â€ï¼Œâ€œé’¢ç´ä¸å¤§æç´â€ï¼Œ
# â€œè¨å…‹æ–¯é£ä¸å°å·â€ï¼Œâ€œå‰ä»–ä¸ç­å“ç´â€ï¼Œâ€œå‰ä»–ä¸åˆæˆå™¨â€ï¼Œ
# â€œè¨å…‹æ–¯é£â€ï¼Œâ€œå°æç´ä¸é’¢ç´â€ï¼Œâ€œåˆæˆå™¨ä¸è´æ–¯â€ï¼Œ
# â€œåˆæˆå™¨ä¸ç”µå‰ä»–â€ï¼Œâ€œç”µå‰ä»–ä¸é’¢ç´â€ï¼Œ
# â€œèŠ‚æ‹ä¸é’¢ç´â€ï¼Œâ€œåˆæˆå™¨ä¸å‰ä»–â€
INSTRUMENTATIONS = [
    "synthesizer and piano", "piano and drums", "piano and synthesizer",
    "synthesizer and drums", "piano and strings", "guitar and drums",
    "guitar and piano", "piano and double bass", "piano and guitar",
    "acoustic guitar and piano", "acoustic guitar and synthesizer",
    "synthesizer and guitar", "piano and saxophone", "saxophone and piano",
    "piano and violin", "electric guitar and drums", "acoustic guitar and drums",
    "synthesizer", "guitar and fiddle", "guitar and harmonica",
    "synthesizer and acoustic guitar", "beats", "piano",
    "acoustic guitar and fiddle", "brass and piano", "bass and drums",
    "violin", "acoustic guitar and harmonica", "piano and cello",
    "saxophone and trumpet", "guitar and banjo", "guitar and synthesizer",
    "saxophone", "violin and piano", "synthesizer and bass",
    "synthesizer and electric guitar", "electric guitar and piano",
    "beats and piano", "synthesizer and guitar"
]

# éŸ³è‰²ï¼šâ€œé»‘æš—çš„â€ã€â€œæ˜äº®çš„â€ã€â€œæ¸©æš–çš„â€ã€â€œå²©çŸ³â€ã€â€œå˜åŒ–çš„â€ã€â€œæŸ”å’Œçš„â€ã€â€œå—“éŸ³â€
TIMBRES = ["dark", "bright", "warm", "rock", "varies", "soft", "vocal"]

AUTO_PROMPT_TYPES = ['Pop', 'R&B', 'Dance', 'Jazz', 'Folk', 'Rock', 
                    'Chinese Style', 'Chinese Tradition', 'Metal', 
                    'Reggae', 'Chinese Opera', 'Auto']

# åˆå§‹åŒ–session state
if 'app_state' not in st.session_state:
    st.session_state.app_state = {
        'lyrics': None,
        'analysis_result': None,
        'singer_gender': SINGER_GENDERS[0],
        'generated_jsonl': None,
        'music_files': []
    }

# ========================
# éŸ³ä¹ç”Ÿæˆæ ¸å¿ƒé€»è¾‘
# ========================
class MusicGenerator:
    def __init__(self, ckpt_path="./ckpt/"):  # ä¿®æ”¹åŸºç¡€è·¯å¾„ä¸ºckptæ ¹ç›®å½•
        self.ckpt_path = ckpt_path
        # æ›´æ–°æ‰€æœ‰æ¨¡å‹è·¯å¾„æŒ‡å‘æ­£ç¡®ä½ç½®
        self.cfg_path = os.path.join(ckpt_path, "songgeneration_base/config.yaml")
        self.model_ckpt = os.path.join(ckpt_path, "songgeneration_base/model.pt")
        self.auto_prompt_path = os.path.join(ckpt_path, "prompt.pt")
        
        # æ·»åŠ éŸ³é¢‘tokenizerè·¯å¾„é…ç½®
        self.audio_tokenizer_checkpoint = os.path.join(ckpt_path, "model_1rvq/model_2_fixed.safetensors")
        self.separate_tokenizer_checkpoint = os.path.join(ckpt_path, "model_septoken/model_2.safetensors")
        
        # æ˜¾å¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å±æ€§ä¸ºNone
        self._audio_tokenizer = None
        self._separate_tokenizer = None
        self._separator = None
        self._model = None
        self._auto_prompt = None
        
        # éªŒè¯å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self._verify_files()
        
    def _verify_files(self):
        required_files = [
            self.cfg_path,
            self.model_ckpt,
            self.auto_prompt_path,
            self.audio_tokenizer_checkpoint,
            self.separate_tokenizer_checkpoint
        ]
        missing = [f for f in required_files if not os.path.exists(f)]
        if missing:
            raise FileNotFoundError(
                f"ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶: {missing}\n"
                "è¯·ç¡®ä¿ï¼š\n"
                "1. æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½\n"
                "2. æ–‡ä»¶è·¯å¾„é…ç½®æ­£ç¡®\n"
                f"å½“å‰æ¨¡å‹ç›®å½•ï¼š{self.ckpt_path}"
            )
        
    @property
    def separator(self):
        if self._separator is None:
            self._separator = Separator()
        return self._separator
    
    @property
    def auto_prompt(self):
        if self._auto_prompt is None and os.path.exists(self.auto_prompt_path):
            self._auto_prompt = torch.load(self.auto_prompt_path)
        return self._auto_prompt
    
    def load_models(self, cfg):
        """åŠ è½½éŸ³é¢‘tokenizerå’Œæ¨¡å‹"""
        if self._audio_tokenizer is None:
            self._audio_tokenizer = builders.get_audio_tokenizer_model(
                cfg.audio_tokenizer_checkpoint, cfg
            ).eval().cuda()
            
        if "audio_tokenizer_checkpoint_sep" in cfg.keys() and self._separate_tokenizer is None:
            self._separate_tokenizer = builders.get_audio_tokenizer_model(
                cfg.audio_tokenizer_checkpoint_sep, cfg
            ).eval().cuda()
            
        return self._audio_tokenizer, self._separate_tokenizer
    
    def generate_music(self, jsonl_path, save_dir="output"):
        """æ‰§è¡ŒéŸ³ä¹ç”Ÿæˆæµç¨‹"""
        # åˆå§‹åŒ–é…ç½®
        torch.backends.cudnn.enabled = False
        OmegaConf.register_new_resolver("eval", lambda x: eval(x))
        OmegaConf.register_new_resolver("concat", lambda *x: [xxx for xx in x for xxx in xx])
        OmegaConf.register_new_resolver("get_fname", lambda: os.path.splitext(os.path.basename(jsonl_path))[0])
        
        cfg = OmegaConf.load(self.cfg_path)
        cfg.mode = 'inference'
        max_duration = cfg.max_dur
        
        # åŠ è½½æ¨¡å‹
        audio_tokenizer, separate_tokenizer = self.load_models(cfg)
        
        # å¤„ç†è¾“å…¥æ•°æ®
        new_items = self.process_input_items(jsonl_path, save_dir, cfg, audio_tokenizer, separate_tokenizer)
        
        # ç”ŸæˆéŸ³ä¹
        self.run_generation(new_items, cfg, max_duration, save_dir)
        
        # æ¸…ç†èµ„æº
        self.cleanup()
        
        # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        return [item['wav_path'] for item in new_items]
    
    def process_input_items(self, jsonl_path, save_dir, cfg, audio_tokenizer, separate_tokenizer):
        """å¤„ç†è¾“å…¥JSONLæ–‡ä»¶"""
        with open(jsonl_path, "r") as fp:
            lines = fp.readlines()
            
        new_items = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            try:
                item = json.loads(line)
            except json.JSONDecoderError:
                st.error(f"Invalid JSON line: {line}")
                continue
                
            target_wav_name = f"{save_dir}/audios/{item['idx']}.flac"
            
            # å¤„ç†æç¤ºéŸ³é¢‘
            if "prompt_audio_path" in item:
                pmt_wav, vocal_wav, bgm_wav = self.process_audio_prompt(item, audio_tokenizer, separate_tokenizer)
            elif "auto_prompt_audio_type" in item:
                pmt_wav, vocal_wav, bgm_wav = self.process_auto_prompt(item)
            else:
                pmt_wav, vocal_wav, bgm_wav = None, None, None
                
            item.update({
                'pmt_wav': pmt_wav,
                'vocal_wav': vocal_wav,
                'bgm_wav': bgm_wav,
                'melody_is_wav': pmt_wav is not None,
                'idx': f"{item['idx']}",
                'wav_path': target_wav_name
            })
            new_items.append(item)
            
        return new_items
    
    def process_audio_prompt(self, item, audio_tokenizer, separate_tokenizer):
        """å¤„ç†éŸ³é¢‘æç¤º"""
        assert os.path.exists(item['prompt_audio_path']), f"Prompt audio not found: {item['prompt_audio_path']}"
        
        pmt_wav, vocal_wav, bgm_wav = self.separator.run(item['prompt_audio_path'])
        
        # ä¿å­˜åŸå§‹æ³¢å½¢ç”¨äºåç»­å¤„ç†
        item['raw_pmt_wav'] = pmt_wav
        item['raw_vocal_wav'] = vocal_wav
        item['raw_bgm_wav'] = bgm_wav
        
        # ç¼–ç éŸ³é¢‘
        pmt_wav = self.prepare_audio_tensor(pmt_wav)
        vocal_wav = self.prepare_audio_tensor(vocal_wav)
        bgm_wav = self.prepare_audio_tensor(bgm_wav)
        
        pmt_wav = pmt_wav.cuda()
        vocal_wav = vocal_wav.cuda()
        bgm_wav = bgm_wav.cuda()
        
        pmt_wav, _ = audio_tokenizer.encode(pmt_wav)
        if separate_tokenizer is not None:
            vocal_wav, bgm_wav = separate_tokenizer.encode(vocal_wav, bgm_wav)
            
        return pmt_wav, vocal_wav, bgm_wav
    
    def process_auto_prompt(self, item):
        """å¤„ç†è‡ªåŠ¨æç¤º"""
        assert item["auto_prompt_audio_type"] in AUTO_PROMPT_TYPES, f"Invalid auto prompt type: {item['auto_prompt_audio_type']}"
        
        if self.auto_prompt is None:
            raise ValueError("Auto prompt file not found")
            
        if item["auto_prompt_audio_type"] == "Auto":
            prompt_token = self.auto_prompt[np.random.randint(0, len(self.auto_prompt))]
        else:
            prompt_token = self.auto_prompt[item["auto_prompt_audio_type"]][np.random.randint(0, len(self.auto_prompt[item["auto_prompt_audio_type"]]))]
            
        return prompt_token[:,[0],:], prompt_token[:,[1],:], prompt_token[:,[2],:]
    
    def prepare_audio_tensor(self, audio):
        """å‡†å¤‡éŸ³é¢‘å¼ é‡"""
        if audio.dim() == 2:
            audio = audio[None]
        if audio.dim() != 3:
            raise ValueError("Audio should have shape [B, C, T]")
        return audio
    
    def run_generation(self, items, cfg, max_duration, save_dir):
        """è¿è¡ŒéŸ³ä¹ç”Ÿæˆ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(f"{save_dir}/audios", exist_ok=True)
        os.makedirs(f"{save_dir}/jsonl", exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model_light = CodecLM_PL(cfg, self.model_ckpt).eval()
        model_light.audiolm.cfg = cfg
        model = CodecLM(
            name="tmp",
            lm=model_light.audiolm,
            audiotokenizer=None,
            max_duration=max_duration,
            seperate_tokenizer=None,
        )
        model.lm = model.lm.cuda().to(torch.float16)
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        model.set_generation_params(
            duration=max_duration,
            extend_stride=5,
            temperature=0.9,
            cfg_coef=1.5,
            top_k=50,
            top_p=0.0,
            record_tokens=True,
            record_window=50
        )
        
        # ç”ŸæˆéŸ³ä¹
        for item in items:
            self.generate_single_item(item, model)
        
        # æ¸…ç†æ¨¡å‹
        del model
        del model_light
        torch.cuda.empty_cache()
        
        # åˆ†ç¦»éŸ³é¢‘ç”Ÿæˆ
        self.generate_separate_audio(items, cfg, save_dir)
        
        # ä¿å­˜ç»“æœ
        src_jsonl_name = os.path.basename(jsonl_path)
        with open(f"{save_dir}/jsonl/{src_jsonl_name}.jsonl", "w", encoding='utf-8') as fw:
            for item in items:
                # æ¸…ç†ä¸´æ—¶å­—æ®µ
                for key in ['tokens', 'pmt_wav', 'vocal_wav', 'bgm_wav', 'melody_is_wav', 
                          'raw_pmt_wav', 'raw_vocal_wav', 'raw_bgm_wav']:
                    item.pop(key, None)
                fw.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    def generate_single_item(self, item, model):
        """ç”Ÿæˆå•ä¸ªéŸ³ä¹é¡¹"""
        generate_inp = {
            'lyrics': [item["gt_lyric"].replace("  ", " ")],
            'descriptions': [item.get("descriptions")],
            'melody_wavs': item['pmt_wav'],
            'vocal_wavs': item['vocal_wav'],
            'bgm_wavs': item['bgm_wav'],
            'melody_is_wav': item['melody_is_wav'],
        }
        
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            item['tokens'] = model.generate(**generate_inp, return_tokens=True)
    
    def generate_separate_audio(self, items, cfg, save_dir):
        """ç”Ÿæˆåˆ†ç¦»çš„éŸ³é¢‘"""
        separate_tokenizer = builders.get_audio_tokenizer_model(cfg.audio_tokenizer_checkpoint_sep, cfg).eval().cuda()
        
        model = CodecLM(
            name="tmp",
            lm=None,
            audiotokenizer=None,
            max_duration=cfg.max_dur,
            seperate_tokenizer=separate_tokenizer,
        )
        
        for item in items:
            with torch.no_grad():
                if 'raw_pmt_wav' in item:   
                    wav_seperate = model.generate_audio(
                        item['tokens'], 
                        item['raw_pmt_wav'], 
                        item['raw_vocal_wav'], 
                        item['raw_bgm_wav'],
                        chunked=True
                    )
                else:
                    wav_seperate = model.generate_audio(item['tokens'], chunked=True)
                    
            torchaudio.save(item['wav_path'], wav_seperate[0].cpu().float(), cfg.sample_rate)
        
        del model
        torch.cuda.empty_cache()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self._separator = None
        self._audio_tokenizer = None
        self._separate_tokenizer = None
        self._model = None
        torch.cuda.empty_cache()

class Separator:
    """éŸ³é¢‘åˆ†ç¦»å™¨"""
    def __init__(self, dm_model_path='third_party/demucs/ckpt/htdemucs.pth', 
                 dm_config_path='third_party/demucs/ckpt/htdemucs.yaml', gpu_id=0):
        self.device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() and gpu_id < torch.cuda.device_count() else "cpu")
        self.demucs_model = self.init_demucs_model(dm_model_path, dm_config_path)
    
    def init_demucs_model(self, model_path, config_path):
        model = get_model_from_yaml(config_path, model_path)
        model.to(self.device)
        model.eval()
        return model
    
    def load_audio(self, f):
        a, fs = torchaudio.load(f)
        if fs != 48000:
            a = torchaudio.functional.resample(a, fs, 48000)
        if a.shape[-1] >= 48000*10:
            a = a[..., :48000*10]
        else:
            a = torch.cat([a, a], -1)
        return a[:, 0:48000*10]
    
    def run(self, audio_path, output_dir='tmp', ext=".flac"):
        os.makedirs(output_dir, exist_ok=True)
        name = os.path.splitext(os.path.basename(audio_path))[0]
        
        # åˆ†ç¦»éŸ³é¢‘
        drums_path, bass_path, other_path, vocal_path = self.demucs_model.separate(
            audio_path, output_dir, device=self.device
        )
        
        # æ¸…ç†ä¸éœ€è¦çš„è½¨é“
        for path in [drums_path, bass_path, other_path]:
            os.remove(path)
            
        # åŠ è½½å¹¶å¤„ç†éŸ³é¢‘
        full_audio = self.load_audio(audio_path)
        vocal_audio = self.load_audio(vocal_path)
        bgm_audio = full_audio - vocal_audio
        
        return full_audio, vocal_audio, bgm_audio

class AudioEncoder(nn.Module):
    """éŸ³é¢‘ç¼–ç å™¨ç½‘ç»œ"""
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.conv_layers = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=15, stride=5, padding=7),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=15, stride=5, padding=7),
            nn.ReLU(),
            nn.Conv1d(128, 256, kernel_size=15, stride=5, padding=7),
            nn.ReLU()
        )
        self.proj = nn.Linear(256, cfg.embedding_dim)
    def forward(self, x):
        # x: [B, 1, T]
        x = self.conv_layers(x)  # [B, 256, T']
        x = x.permute(0, 2, 1)  # [B, T', 256]
        return self.proj(x)  # [B, T', embedding_dim]
class AudioDecoder(nn.Module):
    """éŸ³é¢‘è§£ç å™¨ç½‘ç»œ""" 
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.proj = nn.Linear(cfg.embedding_dim, 256)
        self.conv_trans_layers = nn.Sequential(
            nn.ConvTranspose1d(256, 128, kernel_size=15, stride=5, padding=7, output_padding=4),
            nn.ReLU(),
            nn.ConvTranspose1d(128, 64, kernel_size=15, stride=5, padding=7, output_padding=4),
            nn.ReLU(),
            nn.ConvTranspose1d(64, 1, kernel_size=15, stride=5, padding=7, output_padding=4),
            nn.Tanh()
        )
    def forward(self, x):
        # x: [B, T', embedding_dim]
        x = self.proj(x)  # [B, T', 256]
        x = x.permute(0, 2, 1)  # [B, 256, T']
        return self.conv_trans_layers(x)  # [B, 1, T]
    
# ========================
# åº”ç”¨ç•Œé¢å‡½æ•°
# ========================
def call_deepseek_api(prompt: str, temperature: float = 0.7, max_tokens: int = 2000) -> str:
    """è°ƒç”¨DeepSeek APIç”Ÿæˆæ­Œè¯"""
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(DEEPSEEK_URL, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return None

def analyze_lyrics(lyrics: str) -> Dict[str, str]:
    """åˆ†ææ­Œè¯å¹¶è¿”å›éŸ³ä¹å‚æ•°å»ºè®®
    
    Args:
        lyrics: è¦åˆ†æçš„æ­Œè¯æ–‡æœ¬
        
    Returns:
        åŒ…å«éŸ³ä¹å‚æ•°çš„å­—å…¸ï¼Œæ ¼å¼ä¸º:
        {
            "emotion": str,
            "genre": str,
            "instrumentation": str,
            "timbre": str,
            "gender_suggestion": str
        }
        
    Raises:
        ValueError: å½“APIè¿”å›æ— æ•ˆç»“æœæ—¶
    """
    prompt = f"""è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼åˆ†ææ­Œè¯ç‰¹å¾ï¼š
    {lyrics}
    
    è¿”å›æ ¼å¼å¿…é¡»ä¸ºï¼š
    {{
        "emotion": "ä»{sorted(EMOTIONS)}ä¸­é€‰æ‹©",
        "genre": "ä»{sorted(GENRES)}ä¸­é€‰æ‹©1-2ç§",
        "instrumentation": "ä»{sorted(INSTRUMENTATIONS)}ä¸­é€‰æ‹©",
        "timbre": "ä»{sorted(TIMBRES)}ä¸­é€‰æ‹©",
        "gender_suggestion": "ä»{sorted(SINGER_GENDERS)}ä¸­é€‰æ‹©"
    }}
    
    æ³¨æ„ï¼š
    1. å¿…é¡»è¿”å›åˆæ³•JSON
    2. æ‰€æœ‰å€¼å¿…é¡»æ¥è‡ªç»™å®šé€‰é¡¹
    3. ä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—"""
    
    max_retries = 3
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            result = call_deepseek_api(
                prompt,
                temperature=0.1,  # é™ä½éšæœºæ€§ç¡®ä¿ç¨³å®šè¾“å‡º
                max_tokens=500
            )
            
            if not result:
                raise ValueError("APIè¿”å›ç©ºç»“æœ")
            
            # é¢„å¤„ç†APIå“åº”
            cleaned_result = result.strip()
            
            # å¤„ç†å¯èƒ½çš„ä»£ç å—æ ‡è®°
            if cleaned_result.startswith("```json"):
                cleaned_result = cleaned_result[7:].strip()
            if cleaned_result.endswith("```"):
                cleaned_result = cleaned_result[:-3].strip()
            
            # è§£æJSON
            analysis = json.loads(cleaned_result)
            
            # éªŒè¯ç»“æœ
            required_keys = ["emotion", "genre", "instrumentation", 
                           "timbre", "gender_suggestion"]
            if not all(key in analysis for key in required_keys):
                raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µï¼Œåº”æœ‰: {required_keys}")
            
            # éªŒè¯å­—æ®µå€¼æœ‰æ•ˆæ€§
            if analysis["emotion"] not in EMOTIONS:
                raise ValueError(f"æ— æ•ˆæƒ…ç»ª: {analysis['emotion']}ï¼Œåº”ä¸º: {EMOTIONS}")
                
            if not any(g in analysis["genre"] for g in GENRES):
                raise ValueError(f"æ— æ•ˆç±»å‹: {analysis['genre']}ï¼Œåº”ä¸º: {GENRES}")
                
            if analysis["instrumentation"] not in INSTRUMENTATIONS:
                raise ValueError(f"æ— æ•ˆä¹å™¨ç»„åˆ: {analysis['instrumentation']}ï¼Œåº”ä¸º: {INSTRUMENTATIONS}")
                
            if analysis["timbre"] not in TIMBRES:
                raise ValueError(f"æ— æ•ˆéŸ³è‰²: {analysis['timbre']}ï¼Œåº”ä¸º: {TIMBRES}")
                
            if analysis["gender_suggestion"] not in SINGER_GENDERS:
                raise ValueError(f"æ— æ•ˆæ€§åˆ«å»ºè®®: {analysis['gender_suggestion']}ï¼Œåº”ä¸º: {SINGER_GENDERS}")
            
            # è¿”å›éªŒè¯é€šè¿‡çš„ç»“æœ
            return {
                "emotion": analysis["emotion"],
                "genre": analysis["genre"],
                "instrumentation": analysis["instrumentation"],
                "timbre": analysis["timbre"],
                "gender_suggestion": analysis["gender_suggestion"]
            }
            
        except json.JSONDecodeError as e:
            last_exception = f"JSONè§£æå¤±è´¥: {str(e)}ï¼ŒåŸå§‹å“åº”: {result}"
            st.warning(f"å°è¯• {attempt + 1}/{max_retries}: {last_exception}")
            continue
            
        except ValueError as e:
            last_exception = str(e)
            st.warning(f"å°è¯• {attempt + 1}/{max_retries}: {last_exception}")
            continue
            
        except Exception as e:
            last_exception = str(e)
            st.warning(f"å°è¯• {attempt + 1}/{max_retries}: æœªçŸ¥é”™è¯¯: {last_exception}")
            continue
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åçš„å¤„ç†
    error_msg = f"æ­Œè¯åˆ†æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°ã€‚æœ€åé”™è¯¯: {last_exception}"
    st.error(error_msg)
    
    # è¿”å›ä¿å®ˆé»˜è®¤å€¼
    return {
        "emotion": "emotional",
        "genre": "pop",
        "instrumentation": "piano and strings",
        "timbre": "warm",
        "gender_suggestion": "female"
    }


# ========================
# è¾…åŠ©å‡½æ•°
# ========================
def generate_jsonl_entries(prefix: str, lyrics: str, analysis: Dict[str, Any]) -> List[Dict]:
    """ç”Ÿæˆæ‰€æœ‰JSONLæ¡ç›®"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    entries = [
        {
            "idx": f"{prefix}_autoprompt_{timestamp}",
            "gt_lyric": lyrics,
            "auto_prompt_audio_type": "Auto"
        },
        {
            "idx": f"{prefix}_noprompt_{timestamp}",
            "gt_lyric": lyrics
        },
        {
            "idx": f"{prefix}_textprompt_{timestamp}",
            "descriptions": (
                f"{analysis['gender_suggestion']}, {analysis['timbre']}, "
                f"{analysis['genre']}, {analysis['emotion']}, "
                f"{analysis['instrumentation']}, the bpm is 125"
            ),
            "gt_lyric": lyrics
        },
        {
            "idx": f"{prefix}_audioprompt_{timestamp}",
            "gt_lyric": lyrics,
            "prompt_audio_path": "input/sample_prompt_audio.wav"
        }
    ]
    
    return entries

def save_jsonl(entries: List[Dict], filename: str) -> str:
    """ä¿å­˜JSONLæ–‡ä»¶"""
    os.makedirs("output", exist_ok=True)
    filepath = os.path.join("output", filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        for entry in entries:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    return filepath

def run_music_generation(jsonl_path: str, output_dir: str = "output"):
    """æ‰§è¡ŒéŸ³ä¹ç”Ÿæˆå‘½ä»¤å¹¶å¤„ç†è¾“å‡º"""
    cmd = [
        "bash",
        "generate_lowmem.sh",
        "ckpt/songgeneration_base/",
        jsonl_path,
        output_dir
    ]
    
    # åˆ›å»ºè¿›åº¦æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    output_container = st.expander("ç”Ÿæˆæ—¥å¿—", expanded=True)
    
    # æ‰§è¡Œå‘½ä»¤
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True
    )
    
    # å®æ—¶æ˜¾ç¤ºè¾“å‡º
    full_output = ""
    while True:
        line = process.stdout.readline()
        if line == '' and process.poll() is not None:
            break
        if line:
            full_output += line
            output_container.code(full_output, language="bash")
            
            # æ›´æ–°è¿›åº¦
            if "Generating:" in line:
                progress_bar.progress(min(100, progress_bar.progress_value + 20))
    
    # å¤„ç†ç»“æœ
    if process.returncode == 0:
        st.success("ğŸµ éŸ³ä¹ç”Ÿæˆå®Œæˆï¼")
        display_generated_files(output_dir)
    else:
        st.error(f"âŒ ç”Ÿæˆå¤±è´¥ (è¿”å›ç : {process.returncode})")
        st.text(full_output)

def display_generated_files(output_dir: str):
    """æ˜¾ç¤ºç”Ÿæˆçš„éŸ³ä¹æ–‡ä»¶"""
    audio_files = list(Path(output_dir).glob("audios/*.flac"))
    if not audio_files:
        st.warning("æœªæ‰¾åˆ°ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶")
        return
    
    st.subheader("ç”Ÿæˆçš„éŸ³ä¹")
    for audio_file in sorted(audio_files):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.audio(str(audio_file))
        with col2:
            with open(audio_file, "rb") as f:
                st.download_button(
                    "ä¸‹è½½",
                    data=f.read(),
                    file_name=audio_file.name,
                    mime="audio/flac"
                )

# åœ¨å¸¸é‡å®šä¹‰éƒ¨åˆ†æ·»åŠ éŸ³ä¹æ®µè½æ—¶é•¿é…ç½®
MUSIC_SECTION_TEMPLATES = {
    # çº¯å™¨ä¹æ®µè½
    "intro-short": {
        "description": "å‰å¥è¶…çŸ­ç‰ˆ(0-10ç§’)",
        "duration": "5-10ç§’",
        "lyric_required": False
    },
    "intro-medium": {
        "description": "å‰å¥ä¸­ç­‰ç‰ˆ(10-20ç§’)",
        "duration": "15-20ç§’",
        "lyric_required": False
    },
    "intro-long": {
        "description": "å‰å¥å®Œæ•´ç‰ˆ(20-30ç§’)",
        "duration": "20-30ç§’",
        "lyric_required": False
    },
    "outro-short": {
        "description": "å°¾å¥è¶…çŸ­ç‰ˆ(0-10ç§’)", 
        "duration": "5-10ç§’",
        "lyric_required": False
    },
    "outro-medium": {
        "description": "å°¾å¥ä¸­ç­‰ç‰ˆ(10-20ç§’)",
        "duration": "15-20ç§’",
        "lyric_required": False
    },
    "outro-long": {
        "description": "å°¾å¥å®Œæ•´ç‰ˆ(20-30ç§’)",
        "duration": "20-30ç§’",
        "lyric_required": False
    },
    "inst-short": {
        "description": "é—´å¥çŸ­ç‰ˆ(5-10ç§’)",
        "duration": "5-10ç§’",
        "lyric_required": False
    },
    "inst-medium": {
        "description": "é—´å¥ä¸­ç­‰ç‰ˆ(10-20ç§’)",
        "duration": "15-20ç§’",
        "lyric_required": False
    },
    "inst-long": {
        "description": "é—´å¥å®Œæ•´ç‰ˆ(20-30ç§’)",
        "duration": "20-30ç§’",
        "lyric_required": False
    },
    "silence": {
        "description": "ç©ºç™½åœé¡¿(1-3ç§’)",
        "duration": "1-3ç§’",
        "lyric_required": False
    },
    
    # äººå£°æ®µè½
    "verse": {
        "description": "ä¸»æ­Œæ®µè½(20-30ç§’)",
        "duration": "20-30ç§’",
        "lyric_required": True,
        "lines": "4-8è¡Œ"
    },
    "chorus": {
        "description": "å‰¯æ­Œ(é«˜æ½®æ®µè½)", 
        "duration": "20-30ç§’",
        "lyric_required": True,
        "lines": "4-8è¡Œ"
    },
    "bridge": {
        "description": "è¿‡æ¸¡æ¡¥æ®µ",
        "duration": "15-25ç§’",
        "lyric_required": True,
        "lines": "2-4è¡Œ"
    }
}


# - '[verse]'
# - '[chorus]'
# - '[bridge]'
# - '[intro-short]'
# - '[intro-medium]'
# - '[intro-long]'
# - '[outro-short]'
# - '[outro-medium]'
# - '[outro-long]'
# - '[inst-short]'
# - '[inst-medium]'
# - '[inst-long]'
# - '[silence]'

# å…¸å‹ç»“æ„æ¨¡æ¿
# éŸ³ä¹ç»“æ„æ¨¡æ¿åº“ (36ç§)
STRUCTURE_TEMPLATES = {
    # åŸºç¡€æµè¡Œç»“æ„ (5ç§)
    "pop_basic": {
        "name": "æµè¡ŒåŸºç¡€ç»“æ„",
        "sections": ["intro-medium", "verse", "chorus", "verse", "chorus", "outro-medium"]
    },
    "pop_with_bridge": {
        "name": "æµè¡Œå¸¦æ¡¥æ®µç»“æ„", 
        "sections": ["intro-medium", "verse", "chorus", "verse", "chorus", "bridge", "chorus", "outro-medium"]
    },
    "pop_with_prechorus": {
        "name": "æµè¡Œå¸¦é¢„å‰¯æ­Œç»“æ„",
        "sections": ["intro-short", "verse", "verse", "chorus", "verse", "verse", "chorus", "outro-short"]
    },
    "pop_doublechorus": {
        "name": "æµè¡ŒåŒå‰¯æ­Œç»“æ„",
        "sections": ["intro-short", "verse", "chorus", "chorus", "verse", "chorus", "chorus", "outro-short"]
    },
    "pop_postchorus": {
        "name": "æµè¡Œå¸¦åå‰¯æ­Œç»“æ„",
        "sections": ["intro-medium", "verse", "verse", "chorus", "inst-short", "verse", "verse", "chorus", "inst-short", "outro-medium"]
    },
    
    # æ‘‡æ»š/é‡‘å±ç»“æ„ (8ç§)
    "rock_classic": {
        "name": "ç»å…¸æ‘‡æ»šç»“æ„",
        "sections": ["intro-long", "verse", "chorus", "verse", "chorus", "inst-long", "chorus", "outro-long"]
    },
    "metal_progressive": {
        "name": "å‰å«é‡‘å±ç»“æ„",
        "sections": ["intro-long", "verse", "bridge", "chorus", "inst-long", "verse", "bridge", "chorus", "inst-long", "outro-long"]
    },
    "punk": {
        "name": "æœ‹å…‹ç»“æ„",
        "sections": ["intro-short", "verse", "chorus", "verse", "chorus", "bridge", "chorus", "outro-short"]
    },
    "hardrock": {
        "name": "ç¡¬æ‘‡æ»šç»“æ„",
        "sections": ["intro-long", "verse", "chorus", "verse", "chorus", "inst-long", "inst-long", "chorus", "outro-long"]
    },
    "rock_ballad": {
        "name": "æ‘‡æ»šæŠ’æƒ…æ›²ç»“æ„",
        "sections": ["intro-long", "verse", "verse", "chorus", "inst-long", "verse", "chorus", "outro-long"]
    },
    "metalcore": {
        "name": "é‡‘å±æ ¸ç»“æ„",
        "sections": ["intro-short", "verse", "chorus", "verse", "chorus", "inst-short", "chorus", "outro-short"]
    },
    "blues_rock": {
        "name": "è“è°ƒæ‘‡æ»šç»“æ„",
        "sections": ["intro-medium", "verse", "verse", "chorus", "inst-medium", "verse", "chorus", "outro-medium"]
    },
    "rock_instrumental": {
        "name": "æ‘‡æ»šå™¨ä¹æ›²ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-long", "inst-medium", "inst-long", "inst-long", "outro-long"]
    },
    
    # ç”µå­éŸ³ä¹ç»“æ„ (7ç§)
    "edm_builddrop": {
        "name": "EDMæ„å»º-é«˜æ½®ç»“æ„",
        "sections": ["intro-long", "inst-medium", "inst-short", "inst-medium", "inst-medium", "inst-short", "outro-medium"]
    },
    "house": {
        "name": "æµ©å®¤ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-long", "inst-medium", "inst-short", "outro-long"]
    },
    "trance": {
        "name": "è¿·å¹»ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-short", "inst-medium", "inst-medium", "inst-short", "outro-long"]
    },
    "dubstep": {
        "name": "å›å“è´æ–¯ç»“æ„",
        "sections": ["intro-medium", "verse", "inst-short", "inst-medium", "verse", "inst-short", "outro-short"]
    },
    "techno": {
        "name": "ç§‘æŠ€ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-long", "inst-short", "inst-long", "outro-long"]
    },
    "drum_bass": {
        "name": "é¼“æ‰“è´æ–¯ç»“æ„",
        "sections": ["intro-medium", "inst-short", "verse", "inst-short", "inst-medium", "inst-short", "outro-medium"]
    },
    "ambient": {
        "name": "æ°›å›´ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-short", "inst-medium", "outro-long"]
    },
    
    # å˜»å“ˆ/è¯´å”±ç»“æ„ (5ç§)
    "hiphop_classic": {
        "name": "ç»å…¸å˜»å“ˆç»“æ„",
        "sections": ["intro-short", "verse", "chorus", "verse", "chorus", "bridge", "verse", "chorus", "outro-short"]
    },
    "trap": {
        "name": "é™·é˜±ç»“æ„",
        "sections": ["intro-short", "verse", "chorus", "verse", "chorus", "inst-short", "chorus", "outro-short"]
    },
    "rap_storytelling": {
        "name": "å™äº‹è¯´å”±ç»“æ„",
        "sections": ["intro-medium", "verse", "chorus", "verse", "chorus", "verse", "chorus", "outro-medium"]
    },
    "hiphop_jazzy": {
        "name": "çˆµå£«å˜»å“ˆç»“æ„",
        "sections": ["intro-medium", "verse", "chorus", "verse", "chorus", "inst-medium", "chorus", "outro-medium"]
    },
    "rap_battle": {
        "name": "å¯¹æˆ˜è¯´å”±ç»“æ„",
        "sections": ["intro-short", "verse", "verse", "verse", "verse", "outro-short"]
    },
    
    # ä¸­å›½ä¼ ç»Ÿ/æ°‘æ—ç»“æ„ (6ç§)
    "chinese_folk": {
        "name": "ä¸­å›½æ°‘è°£ç»“æ„",
        "sections": ["intro-long", "verse", "inst-medium", "verse", "inst-medium", "outro-long"]
    },
    "chinese_opera": {
        "name": "æˆæ›²ç»“æ„",
        "sections": ["intro-long", "verse", "inst-short", "verse", "inst-medium", "inst-short", "verse", "outro-long"]
    },
    "guqin": {
        "name": "å¤ç´æ›²ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-long", "inst-medium", "outro-long"]
    },
    "ethnic_fusion": {
        "name": "æ°‘æ—èåˆç»“æ„",
        "sections": ["intro-long", "verse", "chorus", "verse", "chorus", "inst-long", "outro-long"]
    },
    "chinese_pop": {
        "name": "ä¸­å›½æµè¡Œç»“æ„",
        "sections": ["intro-medium", "verse", "verse", "chorus", "inst-medium", "verse", "verse", "chorus", "outro-medium"]
    },
    "mongolian_throat": {
        "name": "è’™å¤å‘¼éº¦ç»“æ„",
        "sections": ["intro-long", "verse", "inst-long", "inst-short", "verse", "inst-short", "outro-long"]
    },
    
    # çˆµå£«/è“è°ƒç»“æ„ (5ç§)
    "jazz_standard": {
        "name": "çˆµå£«æ ‡å‡†ç»“æ„",
        "sections": ["intro-medium", "inst-medium", "inst-long", "inst-medium", "inst-medium", "outro-medium"]
    },
    "blues_12bar": {
        "name": "12å°èŠ‚è“è°ƒç»“æ„",
        "sections": ["intro-short", "verse", "verse", "verse", "inst-medium", "verse", "outro-short"]
    },
    "jazz_fusion": {
        "name": "çˆµå£«èåˆç»“æ„",
        "sections": ["intro-long", "inst-medium", "inst-long", "inst-medium", "inst-short", "inst-medium", "outro-long"]
    },
    "bebop": {
        "name": "æ¯”åšæ™®ç»“æ„",
        "sections": ["intro-short", "inst-short", "inst-medium", "inst-long", "inst-medium", "inst-short", "outro-short"]
    },
    "jazz_ballad": {
        "name": "çˆµå£«æŠ’æƒ…æ›²ç»“æ„",
        "sections": ["intro-long", "inst-long", "inst-medium", "inst-long", "outro-long"]
    }
}

# ç‰¹æ®Šæ®µè½è¯´æ˜
SECTION_DEFINITIONS = {
    "skank": "é›·é¬¼ç‰¹æœ‰çš„åæ‹èŠ‚å¥æ®µè½",
    "guitar-solo": "å‰ä»–ç‹¬å¥éƒ¨åˆ†",
    "post-chorus": "å‰¯æ­Œåçš„è®°å¿†ç‚¹æ®µè½",
    "drop": "ç”µå­èˆæ›²çš„é«˜æ½®éƒ¨åˆ†",
    "head": "çˆµå£«ä¹ä¸»é¢˜æ®µè½",
    "ad-lib": "å³å…´æ¼”å”±éƒ¨åˆ†",
    "12bar": "12å°èŠ‚è“è°ƒè¿›è¡Œ",
    "build-up": "ç”µå­ä¹ä¸­çš„æƒ…ç»ªæ„å»ºæ®µè½",
    "breakdown": "ç”µå­ä¹ä¸­çš„åˆ†è§£æ®µè½",
    "call-response": "éæ´²éŸ³ä¹ä¸­çš„å‘¼åº”æ®µè½",
    "copla": "å¼—æ‹‰é—¨æˆˆä¸­çš„æ­Œå”±æ®µè½",
    "falseta": "å¼—æ‹‰é—¨æˆˆå‰ä»–ç‹¬å¥æ®µè½"
}

def clean_generated_lyrics(raw_lyrics: str) -> str:
    """
    Format raw lyrics into the specified structure:
    - Sections separated by ' ; '
    - Each line in vocal sections ends with a period
    - No spaces around periods
    - Instrumental sections without content
    
    Args:
        raw_lyrics: Raw lyrics text with section markers
        
    Returns:
        Formatted string with strict section formatting
    """
    sections = []
    current_section = None
    current_lines = []
    
    for line in raw_lyrics.split('\n'):
        line = line.strip()
        if not line:
            continue
        
        # Detect section headers like [verse]
        section_match = re.match(r'^\[([a-z\-]+)\]$', line)
        if section_match:
            if current_section is not None:
                sections.append((current_section, current_lines))
            current_section = section_match.group(1)
            current_lines = []
        elif current_section is not None:
            # Clean lyric line and add to current section
            cleaned_line = line.replace('ï¼Œ', '.').replace('ã€‚', '.').strip('. ')
            if cleaned_line:
                current_lines.append(cleaned_line)
    
    # Add the final section if exists
    if current_section is not None:
        sections.append((current_section, current_lines))
    
    # Format each section according to its type
    formatted_sections = []
    for section_type, lines in sections:
        if section_type in ['verse', 'chorus', 'bridge']:
            # Vocal sections: join lines with periods
            content = ".".join(line.rstrip('.') for line in lines if line)
            formatted = f"[{section_type}] {content}" if content else f"[{section_type}]"
        else:
            # Instrumental/other sections: no content
            formatted = f"[{section_type}]"
        formatted_sections.append(formatted)
    
    return " ; ".join(formatted_sections)

def replace_chinese_punctuation(text):
    """æ›¿æ¢ä¸­æ–‡æ ‡ç‚¹ä¸ºè‹±æ–‡æ ‡ç‚¹"""
    punctuation_map = {
        'ï¼Œ': ',', 'ã€‚': '.', 'ã€': ',', 'ï¼›': ';', 'ï¼š': ':',
        'ï¼Ÿ': '?', 'ï¼': '!', 'ã€Œ': '"', 'ã€': '"', 'ã€': '"',
        'ã€': '"', 'ï¼ˆ': '(', 'ï¼‰': ')', 'ã€Š': '"', 'ã€‹': '"'
    }
    
    # é€ä¸ªå­—ç¬¦æ›¿æ¢
    result = []
    for char in text:
        if char in punctuation_map:
            # åœ¨æ ‡ç‚¹å‰åæ·»åŠ ç©ºæ ¼ç¡®ä¿åˆ†å‰²
            result.append(f" {punctuation_map[char]} ")
        else:
            result.append(char)
    
    # åˆå¹¶å¹¶æ ‡å‡†åŒ–ç©ºæ ¼
    return re.sub(r'\s+', ' ', "".join(result)).strip()

# ========================
# æ¨¡å‹æ„å»ºå™¨å®šä¹‰
# ========================
class builders:
    """æ¨¡å‹æ„å»ºå·¥å…·ç±»"""
    
    @staticmethod
    def get_audio_tokenizer_model(checkpoint_path: str, cfg: OmegaConf):
        """æ”¯æŒè‡ªåŠ¨è¡¥å…¨ç›¸å¯¹è·¯å¾„"""
        if not os.path.isabs(checkpoint_path):
            checkpoint_path = os.path.join("./ckpt", checkpoint_path)
            
        if not os.path.exists(checkpoint_path):
            # å°è¯•åœ¨å·²çŸ¥è·¯å¾„ä¸­æŸ¥æ‰¾
            possible_locations = [
                "model_1rvq/model_2_fixed.safetensors",
                "models--lengyue233--content-vec-best/snapshots/c0b9ba13db21beaa4053faae94c102ebe326fd68/model.safetensors"
            ]
            for loc in possible_locations:
                test_path = os.path.join("./ckpt", loc)
                if os.path.exists(test_path):
                    checkpoint_path = test_path
                    break
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(
                f"æ— æ³•æ‰¾åˆ°tokenizeræ¨¡å‹æ–‡ä»¶ï¼Œå·²å°è¯•è·¯å¾„: {checkpoint_path}\n"
                f"è¯·æ£€æŸ¥ä»¥ä¸‹ä½ç½®æ˜¯å¦å­˜åœ¨æ¨¡å‹æ–‡ä»¶:\n"
                f"- ckpt/model_1rvq/model_2_fixed.safetensors\n"
                f"- ckpt/models--lengyue233--content-vec-best/.../model.safetensors"
            )
        
        model = AudioTokenizer(cfg)
        state_dict = torch.load(checkpoint_path, map_location='cuda') # cpu -> cuda
        model.load_state_dict(state_dict)
        return model
    
    @staticmethod
    def get_lm_model(cfg: OmegaConf):
        """åŠ è½½è¯­è¨€æ¨¡å‹"""
        model = AudioLM(
            n_vocab=cfg.n_vocab,
            dim=cfg.dim,
            depth=cfg.depth,
            heads=cfg.heads,
            ff_mult=cfg.ff_mult,
            max_seq_len=cfg.max_seq_len,
            use_flash_attn=cfg.get('use_flash_attn', False)
        )
        return model
    
    @staticmethod
    def get_separate_tokenizer_model(checkpoint_path: str, cfg: OmegaConf):
        """åŠ è½½åˆ†ç¦»tokenizeræ¨¡å‹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Separate tokenizer checkpoint not found: {checkpoint_path}")
            
        model = SeparateAudioTokenizer(cfg)
        state_dict = torch.load(checkpoint_path, map_location='cpu')
        model.load_state_dict(state_dict)
        return model

# ========================
# æ ¸å¿ƒæ¨¡å‹ç±»å®šä¹‰
# ========================
class CodecLM(nn.Module):
    """éŸ³ä¹ç”Ÿæˆæ ¸å¿ƒæ¨¡å‹"""
    
    def __init__(self, 
                 name: str, 
                 lm: nn.Module, 
                 audiotokenizer: nn.Module = None,
                 max_duration: float = 30.0,
                 seperate_tokenizer: nn.Module = None):
        super().__init__()
        self.name = name
        self.lm = lm
        self.audiotokenizer = audiotokenizer
        self.seperate_tokenizer = seperate_tokenizer
        self.max_duration = max_duration
        self.sample_rate = 24000  # é»˜è®¤é‡‡æ ·ç‡
        
        # é»˜è®¤ç”Ÿæˆå‚æ•°
        self.generation_params = {
            'duration': max_duration,
            'extend_stride': 5,
            'temperature': 0.9,
            'cfg_coef': 1.5,
            'top_k': 50,
            'top_p': 0.0,
            'record_tokens': True,
            'record_window': 50
        }
        
    def set_generation_params(self, **kwargs):
        """è®¾ç½®ç”Ÿæˆå‚æ•°"""
        self.generation_params.update(kwargs)
        
    def generate(self, 
                lyrics: List[str],
                descriptions: List[str] = None,
                melody_wavs: torch.Tensor = None,
                vocal_wavs: torch.Tensor = None,
                bgm_wavs: torch.Tensor = None,
                melody_is_wav: bool = False,
                return_tokens: bool = False):
        """
        ç”ŸæˆéŸ³ä¹
        
        Args:
            lyrics: æ­Œè¯åˆ—è¡¨
            descriptions: æè¿°æ–‡æœ¬åˆ—è¡¨
            melody_wavs: æ—‹å¾‹éŸ³é¢‘æç¤º
            vocal_wavs: äººå£°éŸ³é¢‘æç¤º
            bgm_wavs: èƒŒæ™¯éŸ³ä¹éŸ³é¢‘æç¤º
            melody_is_wav: æ˜¯å¦ä¸ºåŸå§‹æ³¢å½¢
            return_tokens: æ˜¯å¦è¿”å›tokenåºåˆ—
            
        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æˆ–tokenåºåˆ—
        """
        # å‡†å¤‡è¾“å…¥
        inputs = self._prepare_inputs(
            lyrics, descriptions, 
            melody_wavs, vocal_wavs, bgm_wavs,
            melody_is_wav
        )
        
        # ç”Ÿæˆtoken
        with torch.no_grad():
            tokens = self.lm.generate(**inputs, **self.generation_params)
            
        if return_tokens:
            return tokens
            
        # è§£ç ä¸ºéŸ³é¢‘
        return self.generate_audio(tokens)
    
    def generate_audio(self, 
                      tokens: torch.Tensor,
                      pmt_wav: torch.Tensor = None,
                      vocal_wav: torch.Tensor = None,
                      bgm_wav: torch.Tensor = None,
                      chunked: bool = False):
        """
        ä»tokenç”ŸæˆéŸ³é¢‘
        
        Args:
            tokens: ç”Ÿæˆçš„tokenåºåˆ—
            pmt_wav: åŸå§‹æç¤ºéŸ³é¢‘(ç”¨äºåˆ†ç¦»æ¨¡å‹)
            vocal_wav: åŸå§‹äººå£°éŸ³é¢‘(ç”¨äºåˆ†ç¦»æ¨¡å‹)
            bgm_wav: åŸå§‹èƒŒæ™¯éŸ³é¢‘(ç”¨äºåˆ†ç¦»æ¨¡å‹)
            chunked: æ˜¯å¦åˆ†å—å¤„ç†
            
        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ³¢å½¢
        """
        if self.seperate_tokenizer is not None and pmt_wav is not None:
            # ä½¿ç”¨åˆ†ç¦»æ¨¡å‹ç”Ÿæˆ
            return self._generate_separate_audio(
                tokens, pmt_wav, vocal_wav, bgm_wav, chunked
            )
        elif self.audiotokenizer is not None:
            # ä½¿ç”¨æ™®é€štokenizerç”Ÿæˆ
            return self._generate_normal_audio(tokens, chunked)
        else:
            raise ValueError("No valid tokenizer available for audio generation")
    
    def _generate_normal_audio(self, tokens: torch.Tensor, chunked: bool):
        """ä½¿ç”¨æ™®é€štokenizerç”ŸæˆéŸ³é¢‘"""
        if chunked:
            # åˆ†å—å¤„ç†å¤§éŸ³é¢‘
            chunk_size = 1024  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
            wavs = []
            for i in range(0, tokens.shape[1], chunk_size):
                chunk = tokens[:, i:i+chunk_size]
                wav = self.audiotokenizer.decode(chunk)
                wavs.append(wav)
            return torch.cat(wavs, dim=-1)
        else:
            return self.audiotokenizer.decode(tokens)
    
    def _generate_separate_audio(self, 
                                tokens: torch.Tensor,
                                pmt_wav: torch.Tensor,
                                vocal_wav: torch.Tensor,
                                bgm_wav: torch.Tensor,
                                chunked: bool):
        """ä½¿ç”¨åˆ†ç¦»tokenizerç”ŸæˆéŸ³é¢‘"""
        if chunked:
            # åˆ†å—å¤„ç†å¤§éŸ³é¢‘
            chunk_size = 1024  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
            wavs = []
            for i in range(0, tokens.shape[1], chunk_size):
                chunk = tokens[:, i:i+chunk_size]
                wav = self.seperate_tokenizer.decode(
                    chunk, pmt_wav, vocal_wav, bgm_wav
                )
                wavs.append(wav)
            return torch.cat(wavs, dim=-1)
        else:
            return self.seperate_tokenizer.decode(
                tokens, pmt_wav, vocal_wav, bgm_wav
            )
    
    def _prepare_inputs(self,
                       lyrics: List[str],
                       descriptions: List[str],
                       melody_wavs: torch.Tensor,
                       vocal_wavs: torch.Tensor,
                       bgm_wavs: torch.Tensor,
                       melody_is_wav: bool):
        """å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        inputs = {
            'texts': lyrics,
            'descriptions': descriptions if descriptions else [None] * len(lyrics)
        }
        
        # å¤„ç†éŸ³é¢‘æç¤º
        if melody_wavs is not None:
            if melody_is_wav:
                # åŸå§‹æ³¢å½¢éœ€è¦ç¼–ç 
                melody_tokens = self.audiotokenizer.encode(melody_wavs)
                inputs['melody_tokens'] = melody_tokens
            else:
                # å·²ç»æ˜¯tokenå½¢å¼
                inputs['melody_tokens'] = melody_wavs
                
        if vocal_wavs is not None and bgm_wavs is not None:
            if self.seperate_tokenizer is not None:
                inputs['vocal_tokens'], inputs['bgm_tokens'] = \
                    self.seperate_tokenizer.encode(vocal_wavs, bgm_wavs)
            else:
                inputs['vocal_tokens'] = self.audiotokenizer.encode(vocal_wavs)
                inputs['bgm_tokens'] = self.audiotokenizer.encode(bgm_wavs)
                
        return inputs

class CodecLM_PL(pl.LightningModule):
    """PyTorch Lightningç‰ˆæœ¬çš„CodecLMæ¨¡å‹"""
    
    def __init__(self, cfg: OmegaConf, checkpoint_path: str = None):
        super().__init__()
        self.cfg = cfg
        self.audiolm = builders.get_lm_model(cfg)
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            self.load_checkpoint(checkpoint_path)
            
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=cfg.pad_token_id)
        
    def load_checkpoint(self, path: str):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        state_dict = torch.load(path, map_location='cpu')
        if 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']
        self.audiolm.load_state_dict(state_dict)
        
    def forward(self, 
               texts: List[str],
               descriptions: List[str] = None,
               melody_tokens: torch.Tensor = None,
               vocal_tokens: torch.Tensor = None,
               bgm_tokens: torch.Tensor = None,
               labels: torch.Tensor = None):
        """å‰å‘ä¼ æ’­"""
        return self.audiolm(
            texts=texts,
            descriptions=descriptions,
            melody_tokens=melody_tokens,
            vocal_tokens=vocal_tokens,
            bgm_tokens=bgm_tokens,
            labels=labels
        )
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        inputs = {
            'texts': batch['texts'],
            'descriptions': batch.get('descriptions'),
            'melody_tokens': batch.get('melody_tokens'),
            'vocal_tokens': batch.get('vocal_tokens'),
            'bgm_tokens': batch.get('bgm_tokens'),
            'labels': batch['labels']
        }
        
        outputs = self(**inputs)
        loss = self.loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), 
                           inputs['labels'].view(-1))
        
        self.log('train_loss', loss, prog_bar=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        inputs = {
            'texts': batch['texts'],
            'descriptions': batch.get('descriptions'),
            'melody_tokens': batch.get('melody_tokens'),
            'vocal_tokens': batch.get('vocal_tokens'),
            'bgm_tokens': batch.get('bgm_tokens'),
            'labels': batch['labels']
        }
        
        outputs = self(**inputs)
        loss = self.loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), 
                           inputs['labels'].view(-1))
        
        self.log('val_loss', loss, prog_bar=True)
        return loss
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.cfg.learning_rate,
            weight_decay=self.cfg.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=self.cfg.max_steps,
            eta_min=self.cfg.min_lr
        )
        
        return [optimizer], [scheduler]

class VectorQuantizer(nn.Module):
    """å‘é‡é‡åŒ–å±‚"""
    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.commitment_cost = commitment_cost
        
        # åˆå§‹åŒ–ç æœ¬
        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)
        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)
    def forward(self, inputs):
        # è®¡ç®—è¾“å…¥ä¸ç æœ¬çš„è·ç¦»
        distances = (torch.sum(inputs**2, dim=-1, keepdim=True) 
                    + torch.sum(self.embedding.weight**2, dim=-1)
                    - 2 * torch.matmul(inputs, self.embedding.weight.t()))
        
        # è·å–æœ€è¿‘é‚»ç¼–ç 
        encoding_indices = torch.argmin(distances, dim=-1)
        quantized = self.embedding(encoding_indices)
        
        # è®¡ç®—æŸå¤±
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self.commitment_cost * e_latent_loss
        
        # ç›´é€šä¼°è®¡å™¨
        quantized = inputs + (quantized - inputs).detach()
        
        return quantized, loss, (encoding_indices, distances)
    @torch.no_grad()
    def quantize(self, inputs):
        distances = (torch.sum(inputs**2, dim=-1, keepdim=True) 
                    + torch.sum(self.embedding.weight**2, dim=-1)
                    - 2 * torch.matmul(inputs, self.embedding.weight.t()))
        return torch.argmin(distances, dim=-1)
    
# ========================
# è¾…åŠ©æ¨¡å‹ç±»
# ========================
class AudioTokenizer(nn.Module):
    """éŸ³é¢‘tokenizeræ¨¡å‹ (VQ-VAEæ¶æ„)
    
    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨å¤„ç†ç¼ºå¤±é…ç½®
    - è¯¦ç»†çš„å‚æ•°éªŒè¯
    - æ”¯æŒåŠ¨æ€é‡é…ç½®
    """
    
    def __init__(self, cfg: Optional[OmegaConf] = None):
        super().__init__()
        # åˆå§‹åŒ–å®Œæ•´é…ç½®
        self.cfg = self._build_complete_config(cfg)
        
        # éªŒè¯é…ç½®æœ‰æ•ˆæ€§
        self._validate_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.encoder = AudioEncoder(self.cfg.audio_tokenizer)
        self.decoder = AudioDecoder(self.cfg.audio_tokenizer)
        self.quantizer = VectorQuantizer(
            num_embeddings=self.cfg.audio_tokenizer.num_embeddings,
            embedding_dim=self.cfg.audio_tokenizer.embedding_dim,
            commitment_cost=self.cfg.audio_tokenizer.commitment_cost
        )
    
    def _build_complete_config(self, cfg: Optional[OmegaConf]) -> OmegaConf:
        """æ„å»ºå®Œæ•´é…ç½®ï¼Œåˆå¹¶é»˜è®¤å€¼å’Œç”¨æˆ·é…ç½®"""
        default_config = OmegaConf.create({
            'audio_tokenizer': {
                'embedding_dim': 256,
                'num_embeddings': 1024,
                'commitment_cost': 0.25,
                'in_channels': 1,
                'sample_rate': 24000,
                'encoder': {
                    'channels': [64, 128, 256],
                    'kernel_sizes': [15, 15, 15],
                    'strides': [5, 5, 5]
                },
                'decoder': {
                    'channels': [256, 128, 64, 1],
                    'kernel_sizes': [15, 15, 15],
                    'strides': [5, 5, 5]
                }
            }
        })
        
        if cfg is None:
            return default_config
            
        # åˆå¹¶é…ç½®
        if 'audio_tokenizer' not in cfg:
            # å¤„ç†å¹³é“ºé…ç½®
            merged = OmegaConf.merge(default_config, {'audio_tokenizer': cfg})
        else:
            merged = OmegaConf.merge(default_config, cfg)
            
        return merged
    
    def _validate_config(self) -> None:
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        required_keys = {
            'top_level': ['audio_tokenizer'],
            'audio_tokenizer': [
                'embedding_dim', 'num_embeddings', 'commitment_cost',
                'in_channels', 'sample_rate', 'encoder', 'decoder'
            ],
            'encoder': ['channels', 'kernel_sizes', 'strides'],
            'decoder': ['channels', 'kernel_sizes', 'strides']
        }
        
        errors = []
        
        # æ£€æŸ¥é¡¶çº§é…ç½®
        for section in required_keys['top_level']:
            if section not in self.cfg:
                errors.append(f"Missing top-level section: {section}")
        
        # æ£€æŸ¥audio_tokenizeré…ç½®
        tokenizer_cfg = self.cfg.audio_tokenizer
        for key in required_keys['audio_tokenizer']:
            if key not in tokenizer_cfg:
                errors.append(f"Missing audio_tokenizer.{key}")
        
        # æ£€æŸ¥encoder/decoderé…ç½®
        for component in ['encoder', 'decoder']:
            if component in tokenizer_cfg:
                for key in required_keys[component]:
                    if key not in tokenizer_cfg[component]:
                        errors.append(f"Missing audio_tokenizer.{component}.{key}")
            else:
                errors.append(f"Missing audio_tokenizer.{component} section")
        
        # æ£€æŸ¥å‚æ•°ç»´åº¦ä¸€è‡´æ€§
        if 'encoder' in tokenizer_cfg:
            enc_cfg = tokenizer_cfg.encoder
            if len(enc_cfg.channels) != len(enc_cfg.kernel_sizes) or \
               len(enc_cfg.channels) != len(enc_cfg.strides):
                errors.append("Encoder config mismatch: channels/kernel_sizes/strides must have same length")
            
            if enc_cfg.channels[-1] != self.cfg.audio_tokenizer.embedding_dim:
                errors.append(
                    f"Encoder output channels ({enc_cfg.channels[-1]}) "
                    f"must match embedding_dim ({self.cfg.audio_tokenizer.embedding_dim})"
                )
        
        if errors:
            raise ValueError(
                "Invalid audio tokenizer configuration:\n  - " + 
                "\n  - ".join(errors) + 
                f"\nCurrent config:\n{OmegaConf.to_yaml(self.cfg)}"
            )
    
    def forward(self, x):
        """å¤„ç†éŸ³é¢‘è¾“å…¥"""
        # [å®ç°æ‚¨çš„forwardé€»è¾‘]
        pass
    @property
    def config_summary(self) -> Dict[str, Any]:
        """è·å–é…ç½®æ‘˜è¦"""
        return {
            'embedding_dim': self.cfg.audio_tokenizer.embedding_dim,
            'num_embeddings': self.cfg.audio_tokenizer.num_embeddings,
            'latent_ratio': self._calculate_latent_ratio(),
            'encoder_params': sum(p.numel() for p in self.encoder.parameters()),
            'decoder_params': sum(p.numel() for p in self.decoder.parameters())
        }
    
    def _calculate_latent_ratio(self) -> float:
        """è®¡ç®—æ½œåœ¨ç©ºé—´ä¸‹é‡‡æ ·ç‡"""
        stride_product = 1
        for s in self.cfg.audio_tokenizer.encoder.strides:
            stride_product *= s
        return float(stride_product)
# è¾…åŠ©ç»„ä»¶å®šä¹‰
class AudioEncoder(nn.Module):
    def __init__(self, cfg: OmegaConf):
        super().__init__()
        layers = []
        in_ch = cfg.in_channels
        for out_ch, k, s in zip(cfg.encoder.channels, 
                               cfg.encoder.kernel_sizes,
                               cfg.encoder.strides):
            layers += [
                nn.Conv1d(in_ch, out_ch, k, stride=s, padding=k//2),
                nn.ReLU()
            ]
            in_ch = out_ch
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)
class AudioDecoder(nn.Module):
    def __init__(self, cfg: OmegaConf):
        super().__init__()
        layers = []
        in_ch = cfg.embedding_dim
        for i, (out_ch, k, s) in enumerate(zip(cfg.decoder.channels,
                                             cfg.decoder.kernel_sizes,
                                             cfg.decoder.strides)):
            layers += [
                nn.ConvTranspose1d(in_ch, out_ch, k, stride=s, 
                                 padding=k//2,
                                 output_padding=s-1),
                nn.ReLU() if i < len(cfg.decoder.channels)-2 else nn.Tanh()
            ]
            in_ch = out_ch
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)

class SeparateAudioTokenizer(nn.Module):
    """åˆ†ç¦»éŸ³é¢‘tokenizeræ¨¡å‹"""
    
    def __init__(self, cfg: OmegaConf):
        super().__init__()
        self.cfg = cfg
        self.vocal_encoder = AudioEncoder(cfg)
        self.bgm_encoder = AudioEncoder(cfg)
        self.decoder = AudioDecoder(cfg)
        self.quantizer = VectorQuantizer(
            num_embeddings=cfg.num_embeddings,
            embedding_dim=cfg.embedding_dim,
            commitment_cost=cfg.commitment_cost
        )
        
    def encode(self, 
              vocal: torch.Tensor, 
              bgm: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç¼–ç åˆ†ç¦»çš„éŸ³é¢‘"""
        vocal_z = self.vocal_encoder(vocal)
        bgm_z = self.bgm_encoder(bgm)
        
        vocal_z_q, _, _ = self.quantizer(vocal_z)
        bgm_z_q, _, _ = self.quantizer(bgm_z)
        
        return vocal_z_q, bgm_z_q
    
    def decode(self, 
              tokens: torch.Tensor,
              pmt_wav: torch.Tensor,
              vocal_wav: torch.Tensor,
              bgm_wav: torch.Tensor) -> torch.Tensor:
        """è§£ç tokenä¸ºåˆ†ç¦»çš„éŸ³é¢‘"""
        # è¿™é‡Œå®ç°åˆ†ç¦»éŸ³é¢‘çš„ç‰¹æ®Šè§£ç é€»è¾‘
        # å®é™…å®ç°å¯èƒ½æ›´å¤æ‚ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        mixed = pmt_wav + 0.5 * vocal_wav + 0.3 * bgm_wav
        return mixed

class AudioLM(nn.Module):
    """éŸ³é¢‘è¯­è¨€æ¨¡å‹"""
    
    def __init__(self,
                 n_vocab: int,
                 dim: int,
                 depth: int,
                 heads: int,
                 ff_mult: int,
                 max_seq_len: int,
                 use_flash_attn: bool = False):
        super().__init__()
        self.token_emb = nn.Embedding(n_vocab, dim)
        self.pos_emb = nn.Embedding(max_seq_len, dim)
        
        self.transformer = Transformer(
            dim=dim,
            depth=depth,
            heads=heads,
            ff_mult=ff_mult,
            flash_attn=use_flash_attn
        )
        
        self.to_logits = nn.Linear(dim, n_vocab)
        
    def forward(self,
               texts: List[str],
               descriptions: List[str] = None,
               melody_tokens: torch.Tensor = None,
               vocal_tokens: torch.Tensor = None,
               bgm_tokens: torch.Tensor = None,
               labels: torch.Tensor = None):
        """å‰å‘ä¼ æ’­"""
        # æ–‡æœ¬åµŒå…¥
        text_emb = self._embed_text(texts, descriptions)
        
        # åˆå¹¶æ‰€æœ‰åµŒå…¥
        x = text_emb
        if melody_tokens is not None:
            melody_emb = self.token_emb(melody_tokens)
            x = x + melody_emb
            
        if vocal_tokens is not None and bgm_tokens is not None:
            vocal_emb = self.token_emb(vocal_tokens)
            bgm_emb = self.token_emb(bgm_tokens)
            x = x + 0.5 * vocal_emb + 0.3 * bgm_emb
            
        # ä½ç½®ç¼–ç 
        seq_len = x.shape[1]
        positions = torch.arange(seq_len, device=x.device)
        pos_emb = self.pos_emb(positions)
        x = x + pos_emb
        
        # Transformerå¤„ç†
        x = self.transformer(x)
        
        # è¾“å‡ºlogits
        logits = self.to_logits(x)
        
        # è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=self.cfg.pad_token_id
            )
            
        return {'logits': logits, 'loss': loss}
    
    def generate(self,
                texts: List[str],
                descriptions: List[str] = None,
                melody_tokens: torch.Tensor = None,
                vocal_tokens: torch.Tensor = None,
                bgm_tokens: torch.Tensor = None,
                duration: float = 30.0,
                extend_stride: int = 5,
                temperature: float = 1.0,
                cfg_coef: float = 1.5,
                top_k: int = 50,
                top_p: float = 0.0,
                record_tokens: bool = False,
                record_window: int = 50):
        """ç”ŸæˆéŸ³ä¹token"""
        # åˆå§‹åŒ–ç”ŸæˆçŠ¶æ€
        generated = []
        if record_tokens:
            all_tokens = []
            
        # æ–‡æœ¬åµŒå…¥
        text_emb = self._embed_text(texts, descriptions)
        
        # åˆå§‹è¾“å…¥
        x = text_emb
        if melody_tokens is not None:
            melody_emb = self.token_emb(melody_tokens)
            x = x + melody_emb
            
        if vocal_tokens is not None and bgm_tokens is not None:
            vocal_emb = self.token_emb(vocal_tokens)
            bgm_emb = self.token_emb(bgm_tokens)
            x = x + 0.5 * vocal_emb + 0.3 * bgm_emb
            
        # ç”Ÿæˆå¾ªç¯
        for i in range(int(duration * self.cfg.sample_rate / extend_stride)):
            # ä½ç½®ç¼–ç 
            positions = torch.arange(x.shape[1], device=x.device)
            pos_emb = self.pos_emb(positions)
            x = x + pos_emb
            
            # Transformerå¤„ç†
            x = self.transformer(x)
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            logits = self.to_logits(x[:, -1:])
            next_token = self._sample_token(
                logits, temperature, top_k, top_p, cfg_coef
            )
            
            # æ›´æ–°ç”Ÿæˆåºåˆ—
            generated.append(next_token)


# å…¸å‹ç»“æ„æ¨¡æ¿
# ========================
# Streamlit ç•Œé¢
# ========================
def setup_ui():
    """è®¾ç½®Streamlitç”¨æˆ·ç•Œé¢"""
    st.set_page_config(page_title="MusicFayIn", layout="wide")
    st.title("ğŸµ MusicFayIn äººå·¥æ™ºèƒ½éŸ³ä¹ç”Ÿæˆç³»ç»Ÿ")
    
    # æ­¥éª¤1: æ­Œè¯ç”Ÿæˆ
    st.header("ç¬¬ä¸€æ­¥: ç”Ÿæˆæ­Œè¯")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        lyric_prompt = st.text_area("è¾“å…¥æ­Œè¯ä¸»é¢˜", "å†¬å¤©çš„çˆ±æƒ…æ•…äº‹")
        
        # ç»“æ„æ¨¡æ¿é€‰æ‹©
        selected_template = st.selectbox(
            "é€‰æ‹©æ­Œæ›²ç»“æ„æ¨¡æ¿",
            options=list(STRUCTURE_TEMPLATES.keys()),
            format_func=lambda x: STRUCTURE_TEMPLATES[x]["name"]
        )
        
        # æ˜¾ç¤ºé€‰ä¸­çš„æ¨¡æ¿ç»“æ„
        if selected_template:
            template = STRUCTURE_TEMPLATES[selected_template]
            st.markdown("**å½“å‰ç»“æ„:**")
            for i, section in enumerate(template["sections"]):
                info = MUSIC_SECTION_TEMPLATES[section]
                st.markdown(f"{i+1}. `[{section}]` - {info['description']}")
                
    with col2:
        # æ˜¾ç¤ºæ®µè½æ—¶é•¿è¯´æ˜
        st.markdown("### ğŸµ éŸ³ä¹æ®µè½æ—¶é•¿è§„èŒƒ")
        st.markdown("""
        **ä¸€ã€çº¯å™¨ä¹æ®µè½ï¼ˆä¸å«æ­Œè¯ï¼‰:**
        - `[intro-short]`: å‰å¥è¶…çŸ­ç‰ˆ (0-10ç§’)
        - `[outro-short]`: å°¾å¥è¶…çŸ­ç‰ˆ (0-10ç§’)
        - `[intro-medium]`: å‰å¥ä¸­ç­‰ç‰ˆ (10-20ç§’)
        - `[outro-medium]`: å°¾å¥ä¸­ç­‰ç‰ˆ (10-20ç§’)
        
        **äºŒã€äººå£°æ®µè½ï¼ˆå¿…é¡»å«æ­Œè¯ï¼‰:**
        - `[verse]`: ä¸»æ­Œ (20-30ç§’, 4-8è¡Œ)
        - `[chorus]`: å‰¯æ­Œ (é«˜æ½®æ®µè½, 20-30ç§’)
        - `[bridge]`: è¿‡æ¸¡æ¡¥æ®µ (15-25ç§’, 2-4è¡Œ)
        """)
        
        st.markdown("### ğŸ“ ä½¿ç”¨å»ºè®®")
        st.markdown("""
        - å™¨ä¹æ®µè½ä¸¥æ ¼æŒ‰ç§’æ•°èŒƒå›´æ§åˆ¶
        - äººå£°æ®µè½é€šè¿‡æ­Œè¯è¡Œæ•°æ§åˆ¶æ—¶é•¿
        - å‰å¥/å°¾å¥è‹¥è¶…è¿‡20ç§’ï¼Œå¯ç»„åˆä½¿ç”¨:
          `[intro-medium][intro-short]` â‰ˆ 25ç§’
        """)
    # ç”Ÿæˆæ­Œè¯æŒ‰é’®
    if st.button("ç”Ÿæˆæ­Œè¯"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆæ­Œè¯..."):
            # è·å–é€‰ä¸­çš„æ¨¡æ¿ç»“æ„
            template = STRUCTURE_TEMPLATES[selected_template]
            
            # æ„å»ºè¯¦ç»†çš„æç¤ºè¯
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ç”Ÿæˆä¸€é¦–ä¸­æ–‡æ­Œæ›²çš„å®Œæ•´æ­Œè¯ï¼š
                        
            ä¸»é¢˜ï¼š{lyric_prompt}
            æ­Œæ›²ç»“æ„ï¼š
            {", ".join([f"[{section}]" for section in template["sections"]])}
            å…·ä½“è¦æ±‚ï¼š
            1. ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„ç»“æ„æ ‡ç­¾åˆ†æ®µ
            2. å™¨ä¹æ®µè½([intro-*]/[outro-*])ä¸éœ€è¦å¡«æ­Œè¯
            3. äººå£°æ®µè½([verse]/[chorus]/[bridge])å¿…é¡»åŒ…å«æ­Œè¯
            4. ä¸»æ­Œ([verse])æ¯æ®µ4-8è¡Œ
            5. å‰¯æ­Œ([chorus])è¦çªå‡ºé«˜æ½®éƒ¨åˆ†
            6. æ¡¥æ®µ([bridge])2-4è¡Œ
            7. æ•´ä½“è¦æœ‰æŠ¼éŸµå’ŒèŠ‚å¥æ„Ÿ
            8. ä¸è¦åŒ…å«æ­Œæ›²æ ‡é¢˜
            9. ä¸è¦åŒ…å«éŸµè„šåˆ†æç­‰é¢å¤–è¯´æ˜
            è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
            [intro-medium]
            [verse]
            ç¬¬ä¸€è¡Œæ­Œè¯
            ç¬¬äºŒè¡Œæ­Œè¯
            ...
            [chorus]
            å‰¯æ­Œç¬¬ä¸€è¡Œ
            å‰¯æ­Œç¬¬äºŒè¡Œ
            ...
            """
            lyrics = call_deepseek_api(prompt)
            if lyrics:
                cleaned_lyrics = clean_generated_lyrics(lyrics)
                st.session_state.app_state['lyrics'] = cleaned_lyrics
                st.text_area("ç”Ÿæˆçš„æ­Œè¯", cleaned_lyrics, height=400)
                
                # è‡ªåŠ¨åˆ†ææ­Œè¯å‚æ•°
                with st.spinner("æ­£åœ¨åˆ†ææ­Œè¯ç‰¹å¾..."):
                    analysis = analyze_lyrics(cleaned_lyrics)
                    if analysis:
                        st.session_state.app_state['analysis_result'] = analysis
                        st.success("æ­Œè¯åˆ†æå®Œæˆï¼")

    # æ­¥éª¤2: åˆ†ææ­Œè¯
    if st.session_state.app_state.get('lyrics'):
        st.header("ç¬¬äºŒæ­¥: åˆ†ææ­Œè¯")
        
        if st.button("åˆ†ææ­Œè¯å‚æ•°"):
            with st.spinner("æ­£åœ¨åˆ†ææ­Œè¯..."):
                analysis = analyze_lyrics(st.session_state.app_state['lyrics'])
                if analysis:
                    st.session_state.app_state['analysis_result'] = analysis
                    st.json(analysis)

    # æ­¥éª¤3: å‚æ•°è°ƒæ•´
    if st.session_state.app_state.get('analysis_result'):
        st.header("ç¬¬ä¸‰æ­¥: å‚æ•°è°ƒæ•´")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ä½¿ç”¨åˆ†æç»“æœæˆ–æä¾›é»˜è®¤å€¼
            default_gender = st.session_state.app_state['analysis_result'].get(
                'gender_suggestion', SINGER_GENDERS[0]
            )
            st.session_state.app_state['singer_gender'] = st.radio(
                "æ­Œæ‰‹æ€§åˆ«", SINGER_GENDERS,
                index=SINGER_GENDERS.index(default_gender),
                horizontal=True
            )
            
            default_emotion = st.session_state.app_state['analysis_result'].get(
                'emotion', EMOTIONS[0]
            )
            st.session_state.app_state['analysis_result']['emotion'] = st.selectbox(
                "æƒ…ç»ª", EMOTIONS,
                index=EMOTIONS.index(default_emotion)
            )
            
            default_timbre = st.session_state.app_state['analysis_result'].get(
                'timbre', TIMBRES[0]
            )
            st.session_state.app_state['analysis_result']['timbre'] = st.selectbox(
                "éŸ³è‰²", TIMBRES,
                index=TIMBRES.index(default_timbre)
            )
        
        with col2:
            default_genre = st.session_state.app_state['analysis_result'].get(
                'genre', GENRES[0]
            )
            st.session_state.app_state['analysis_result']['genre'] = st.selectbox(
                "æ­Œæ›²ç±»å‹", GENRES,
                index=GENRES.index(default_genre.split(",")[0])  # å–ç¬¬ä¸€ä¸ªç±»å‹
            )
            
            default_instrument = st.session_state.app_state['analysis_result'].get(
                'instrumentation', INSTRUMENTATIONS[0]
            )
            st.session_state.app_state['analysis_result']['instrumentation'] = st.selectbox(
                "ä¹å™¨ç»„åˆ", INSTRUMENTATIONS,
                index=INSTRUMENTATIONS.index(default_instrument)
            )

    # æ­¥éª¤4: ç”ŸæˆJSONL
    if st.session_state.app_state.get('analysis_result'):
        st.header("ç¬¬å››æ­¥: ç”Ÿæˆé…ç½®")
        
        prefix = st.text_input("IDå‰ç¼€", "sample_01")
        
        if st.button("ç”ŸæˆJSONLé…ç½®"):
            entries = generate_jsonl_entries(
                prefix,
                st.session_state.app_state['lyrics'],
                st.session_state.app_state['analysis_result']
            )
            
            filename = f"{prefix}_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
            filepath = save_jsonl(entries, filename)
            
            st.session_state.app_state['generated_jsonl'] = filepath
            st.success(f"JSONLæ–‡ä»¶å·²ç”Ÿæˆ: {filepath}")
            
            for entry in entries:
                st.json(entry)

    # æ­¥éª¤5: ç”ŸæˆéŸ³ä¹
    if st.session_state.app_state.get('generated_jsonl'):
        st.header("ç¬¬äº”æ­¥: ç”ŸæˆéŸ³ä¹")
        
        # è¾“å‡ºç›®å½•è®¾ç½®
        output_dir = st.text_input("è¾“å‡ºç›®å½•", "output")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        try:
            # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = [
                "ckpt/songgeneration_base/config.yaml",
                "ckpt/songgeneration_base/model.pt",
                "ckpt/model_1rvq/model_2_fixed.safetensors",
                "ckpt/model_septoken/model_2.safetensors",
                "ckpt/prompt.pt"
            ]
            
            missing_files = [f for f in required_files if not os.path.exists(f)]
            
            if missing_files:
                raise FileNotFoundError(
                    f"ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶:\n{chr(10).join(missing_files)}\n"
                    "è¯·ç¡®ä¿æ–‡ä»¶ç»“æ„å¦‚ä¸‹:\n"
                    "ckpt/\n"
                    "â”œâ”€â”€ model_1rvq/\n"
                    "â”‚   â””â”€â”€ model_2_fixed.safetensors\n"
                    "â”œâ”€â”€ model_septoken/\n"
                    "â”‚   â””â”€â”€ model_2.safetensors\n"
                    "â”œâ”€â”€ prompt.pt\n"
                    "â””â”€â”€ songgeneration_base/\n"
                    "    â”œâ”€â”€ config.yaml\n"
                    "    â””â”€â”€ model.pt"
                )
            
            st.success("âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡")
            
            if st.button("è¿è¡ŒéŸ³ä¹ç”Ÿæˆ"):
                # å‡†å¤‡ç”Ÿæˆå‘½ä»¤
                jsonl_path = st.session_state.app_state['generated_jsonl']
                cmd = [
                    "bash", 
                    "generate_lowmem.sh",
                    "ckpt/songgeneration_base/",
                    jsonl_path,
                    output_dir
                ]
                
                # æ˜¾ç¤ºæ‰§è¡Œçš„å‘½ä»¤
                st.code(" ".join(cmd), language="bash")
                
                # åˆ›å»ºè¿›åº¦æ¡å’Œè¾“å‡ºå®¹å™¨
                progress_bar = st.progress(0)
                output_container = st.empty()
                
                # æ‰§è¡Œç”Ÿæˆå‘½ä»¤
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    universal_newlines=True
                )
                
                # å®æ—¶æ˜¾ç¤ºè¾“å‡º
                full_output = ""
                progress_value = 0  # åˆå§‹åŒ–è¿›åº¦å€¼
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        full_output += output
                        output_container.text(full_output)
                        progress_value = min(100, progress_value + 5)  # æ›´æ–°è¿›åº¦å€¼
                        progress_bar.progress(progress_value)  # ä½¿ç”¨æ›´æ–°åçš„å€¼
                
                # æ£€æŸ¥æ‰§è¡Œç»“æœ
                return_code = process.poll()
                if return_code == 0:
                    st.success("ğŸµ éŸ³ä¹ç”Ÿæˆå®Œæˆï¼")
                    
                    # æ˜¾ç¤ºç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
                    audio_files = glob.glob(f"{output_dir}/audios/*.flac")
                    if audio_files:
                        st.subheader("ç”Ÿæˆçš„éŸ³ä¹æ–‡ä»¶")
                        for audio_file in sorted(audio_files):
                            st.audio(audio_file)
                            st.download_button(
                                f"ä¸‹è½½ {os.path.basename(audio_file)}",
                                data=open(audio_file, "rb").read(),
                                file_name=os.path.basename(audio_file),
                                mime="audio/flac"
                            )
                    else:
                        st.warning("æœªæ‰¾åˆ°ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶")
                else:
                    st.error(f"âŒ ç”Ÿæˆå¤±è´¥ (è¿”å›ç : {return_code})")
                    st.text(full_output)
                    
        except FileNotFoundError as e:
            st.error(str(e))
            st.warning("è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²æ­£ç¡®ä¸‹è½½å¹¶æ”¾ç½®åœ¨æŒ‡å®šä½ç½®")
        except Exception as e:
            st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


    # ä¾§è¾¹æ è¯´æ˜
    st.sidebar.markdown("""
    ### ä½¿ç”¨æµç¨‹
    1. **ç”Ÿæˆæ­Œè¯**ï¼šè¾“å…¥ä¸»é¢˜ç”Ÿæˆæ­Œè¯
    2. **åˆ†ææ­Œè¯**ï¼šè‡ªåŠ¨åˆ†æéŸ³ä¹å‚æ•°
    3. **è°ƒæ•´å‚æ•°**ï¼šæ ¹æ®éœ€è¦ä¿®æ”¹å‚æ•°
    4. **ç”Ÿæˆé…ç½®**ï¼šåˆ›å»ºJSONLé…ç½®æ–‡ä»¶
    5. **ç”ŸæˆéŸ³ä¹**ï¼šè¿è¡Œç”Ÿæˆè„šæœ¬

    ### ç”Ÿæˆé€‰é¡¹
    - è‡ªåŠ¨ç”Ÿæˆ (autoprompt)
    - æ— æç¤ºç”Ÿæˆ (noprompt)
    - æ–‡æœ¬æç¤ºç”Ÿæˆ (textprompt)
    - éŸ³é¢‘æç¤ºç”Ÿæˆ (audioprompt)
    """)

    # ç³»ç»Ÿç›‘æ§
    if st.sidebar.checkbox("æ˜¾ç¤ºç³»ç»Ÿèµ„æº"):
        show_system_monitor()

def show_system_monitor():
    """æ˜¾ç¤ºç³»ç»Ÿèµ„æºç›‘æ§"""
    st.sidebar.subheader("ç³»ç»Ÿèµ„æºç›‘æ§")
    
    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent()
    st.sidebar.metric("CPUä½¿ç”¨ç‡", f"{cpu_percent}%")
    st.sidebar.progress(cpu_percent / 100)
    
    # å†…å­˜ä½¿ç”¨
    mem = psutil.virtual_memory()
    st.sidebar.metric("å†…å­˜ä½¿ç”¨", 
                     f"{mem.used/1024/1024:.1f}MB / {mem.total/1024/1024:.1f}MB",
                     f"{mem.percent}%")
    
    # ç£ç›˜ç©ºé—´
    disk = psutil.disk_usage('/')
    st.sidebar.metric("ç£ç›˜ç©ºé—´", 
                     f"{disk.used/1024/1024:.1f}MB / {disk.total/1024/1024:.1f}MB",
                     f"{disk.percent}%")
    
    # GPUä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        st.sidebar.subheader("GPUä¿¡æ¯")
        for i in range(torch.cuda.device_count()):
            mem = torch.cuda.mem_get_info(i)
            total = mem[1] / 1024**3
            free = mem[0] / 1024**3
            used = total - free
            st.sidebar.metric(
                f"GPU {i} ({torch.cuda.get_device_name(i)})",
                f"{used:.1f}GB / {total:.1f}GB",
                f"{used/total*100:.1f}%"
            )

# ========================
# ä¸»ç¨‹åº
# ========================
if __name__ == "__main__":
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    os.makedirs("output/audios", exist_ok=True)
    os.makedirs("output/jsonl", exist_ok=True)
    os.makedirs("input", exist_ok=True)
    
    # è®¾ç½®å¹¶è¿è¡ŒUI
    setup_ui()
